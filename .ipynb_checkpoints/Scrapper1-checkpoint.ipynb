{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEBSCRAPING PROJECT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![WEB IMAGE](web-scraping-with-python.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION\n",
    "Webscrapping is the process of extracting data from a website and tranforming this data to a more useful structure that can \n",
    "uderstood by a user or an API.\n",
    "Webscrapping is one of the recquired skills  in the data collection stage and therefore this project is a simple task that i  \n",
    "have taken to understand webscrapping.\n",
    "It involves scrapping wikipedia pages and extracting some information which is then stored in a json file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTING PACKAGES\n",
    "In this stage I import all the necessary packages that i need to work out this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup as bs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REGIONS\n",
    "In this first section, i scrape through the wikipedia website containing regions in uganda and i save the result in a \n",
    "json file containing \n",
    "- id:\n",
    "- Region:\n",
    "- area:\n",
    "- chief_town:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'https://en.wikipedia.org/wiki/Regions_of_Uganda'\n",
    "url1_text = requests.get('https://en.wikipedia.org/wiki/Regions_of_Uganda').text\n",
    "soup = bs(url1_text, 'html.parser')\n",
    "table = soup.find('table', class_ = 'wikitable sortable')\n",
    "headers = [th.text.strip() for th in table.tr.find_all('th')]\n",
    "\n",
    "lst_data = []\n",
    "for index,row in enumerate(table.tbody.find_all('tr')[1:5]):\n",
    "    data = [td.text.strip() for td in row.find_all('td')]\n",
    "    lst_data.append(data)\n",
    "    \n",
    "dirty_df = pd.DataFrame(data=lst_data, columns=headers)\n",
    "dirty_df.drop(columns=['Population(Census 1991)','Population(Census 2002)','Population(Census 2014)','Number of Districts'],inplace=True)\n",
    "Region_df = dirty_df.reset_index().rename(columns = ({'index':'Reg_ID'}))\n",
    "Region_df.set_index('Reg_ID',inplace = True)\n",
    "regions = Region_df.to_json(orient = 'records')\n",
    "\n",
    "with open('regions.json','w+') as f:\n",
    "    f.write(regions)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DISTRICTS\n",
    "In this section, i scrape through a wikipedia page containing a number of tables on districts in Uganda and save the data\n",
    "to a json file containing:\n",
    "- id\n",
    "- name\n",
    "- region_id\n",
    "- 2014_population\n",
    "- 2023_population_est\n",
    "- area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = 'https://en.wikipedia.org/wiki/Districts_of_Uganda'\n",
    "url2_text = requests.get('https://en.wikipedia.org/wiki/Districts_of_Uganda').text\n",
    "soup = bs(url2_text,'html.parser')\n",
    "dis_data = []\n",
    "for table in soup.find_all('table','wikitable sortable'):\n",
    "    headerss = [th.text.strip() for th in table.tbody.tr.find_all('th')]\n",
    "    for index,row in enumerate(table.tbody.find_all('tr')[1:-1]):\n",
    "        data = [td.text.strip() for td in row.find_all('td')]\n",
    "        dis_data.append(data)\n",
    "dirty_df2 = pd.DataFrame(data =dis_data,columns =headerss)\n",
    "dirty_df2.rename(columns={'Map':'District_ID'},inplace = True)\n",
    "dirty_df2['District_ID'] = dirty_df2['District_ID'].astype('int')\n",
    "dirty_df2.sort_values(by='District_ID',ascending = True, inplace = True, ignore_index = True)\n",
    "dirty_df2\n",
    "def Region_Identification(district):\n",
    "    if district <=38 :\n",
    "        return '3'\n",
    "    if district == 39 or district <= 75:\n",
    "        return '2'\n",
    "    if district == 76 or district <= 101:\n",
    "        return '0'\n",
    "    if district > 101:\n",
    "        return '1'\n",
    "        \n",
    "dirty_df2['Reg_ID'] = dirty_df2['District_ID'].apply(Region_Identification)\n",
    "# dirty_df2.set_index('District_ID',inplace = True)\n",
    "cols =['District_ID','Reg_ID','Pop (2014)','Pop(2023 est.)','Area (km2)']\n",
    "dirty = dirty_df2[cols]\n",
    "dirty\n",
    "districts = dirty.to_json(orient='records')\n",
    "with open('districts.json','w+') as f:\n",
    "    f.write(districts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTITUTIIONS\n",
    "In this project, i scrape through wikipedia pages of different institutions and save the following information in \n",
    "json file.\n",
    "- id\n",
    "- name:\n",
    "- district_id:\n",
    "- ownership_type: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "url3 = 'https://unche.or.ug/all-academic-programs/'\n",
    "url3_text = requests.get('https://unche.or.ug/all-academic-programs/').text\n",
    "soup = bs(url3_text,'html.parser')\n",
    "insti_table = soup.find('table', id = 'unche-table')\n",
    "headersss =[th.text.strip() for th in insti_table.tr.find_all('th')]\n",
    "\n",
    "institution_data = []\n",
    "for index,row in enumerate(insti_table.tbody.find_all('tr')):\n",
    "    dara = [td.text.strip() for td in row.find_all('td')]\n",
    "    institution_data.append(dara)\n",
    "\n",
    "dirty_df3 = pd.DataFrame(data = institution_data, columns = headersss)\n",
    "dirty_df3 = dirty_df3.reset_index().rename(columns = ({'index':'id'}))\n",
    "def check_ownership(University):\n",
    "    public = ['Bukalasa Agricultural College','Busitema University','Busitema University','Mbale Campus','Busitema University', 'Nagongera Campus',\n",
    "'Busitema University Pallisa Campus','Butabika Psychiatric Nursing School','Butabika School of Psychiatric Clinical Officers',\n",
    "'College of Health Sciences','Makerere University','East African School of Aviation','Fisheries Training Institute','Fortportal College of Health Sciences',\n",
    "'Gulu College of Health Sciences','Gulu University','Health Tutors College Mulago','Institute of Survey and Land Management'\n",
    "'Jinja School of Nursing and Midwifery Junior Staff College','Kabale University','Kyambogo University','Law Development Centre Kampala',\n",
    "'Lira University','Makerere University','Makerere University Business School','Makerere University Business School Arua Campus',\n",
    "'Makerere University Business School, Jinja Campus','Makerere University Business School, Mbarara','Makerere University College of Agriculture and Environmental Sciences',\n",
    "'Makerere University College of Business and Management Science','Makerere University College of Computing and Information Science',\n",
    "'Makerere University College of Engineering, design, Art and Technology','Makerere University College of Humanities and Social Sciences',\n",
    "'Makerere University College of Natural Sciences','Makerere University College of Veterinary Medicine, Animal Resources and Bio security',\n",
    "'Makerere University, Jinja Campus','Management Training and Advisory Centre (MTAC)','Masaka School of Comprehensive Nursing',\n",
    "'Mbale College of Health Sciences','Mbale School of Midwifery and Nursing','Mbarara University of Science and Technology','Medical Laboratory Technician’s School',\n",
    "'Mountains of the Moon University','Mulago Medical Laboratory School','Mulago School of Dispensing /Pharmacy','Mulago School of Nursing and Midwifery',\n",
    "'Mulago School of Occupational Therapy','Mulago School of Physiotherapy','Mulago School of Radiography','Muni University','Nakawa Vocational Training Institute',\n",
    "'National Meteorological Training School','National Teachers College, Kabale','National Teachers College, Kaliro','National Teachers College, Mubende',\n",
    "'National Teachers College, Muni','National Teachers College, Unyama','Non – Commissioned Officer Academy','Nsamizi Training Institute of Social Development',\n",
    "'Nyabyeya Forestry College, Masindi','Ophthalmic Clinical Officers Training School','Public Health Nurses’ College','School of Hygiene, Mbale',\n",
    "'Senior Command and Staff College','Soroti School of Comprehensive Nursing','Soroti University','The Uganda Hotel and Tourism Training Institute',\n",
    "'Uganda College of Commerce, Aduku','Uganda College of Commerce, Kabale','Uganda College of Commerce, Pakwach','Uganda College of Commerce, Soroti',\n",
    "'Uganda College of Commerce, Tororo','Uganda Cooperative College Kigumba','Uganda Cooperatives College- Tororo','Uganda Institute of Allied Health and Management Science (UIAHMS) Mulago',\n",
    "'Uganda Institute of Allied Health and Management Sciences','Uganda Institute of Information and Communications Technology','Uganda Management Institute',\n",
    "'Uganda Management Institute, Gulu Campus','Uganda Management Institute, Mbale Campus','Uganda Management Institute, Mbarara Campus',\n",
    "'Uganda Military Academy (UMA)','Uganda Petroleum Institute Kigumba','Uganda Technical College, Bushenyi','Uganda Technical College, Elgon',\n",
    "'Uganda Technical College, Kichwamba','Uganda Technical College, Kyema,','Uganda Technical College, Lira','Uganda Wildlife Research and Training Institute'\n",
    "]\n",
    "    if University in public:\n",
    "        return 'PUBLIC'\n",
    "    else:\n",
    "        return 'PRIVATE'\n",
    "dirty_df3['Ownership_type'] = dirty_df3['Institution Name'].apply(check_ownership)\n",
    "dirty_df3.drop(['Program Name','Accredited Date','Expiry Date','Status'],axis =1 ,inplace = True)\n",
    "institutions = dirty_df3.to_json(orient='records')\n",
    "with open('institutions.json','w+') as f:\n",
    "    f.write(institutions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROGRAMS \n",
    "In this section a i scrape through the wikipedia page containing different programs and save the following information \n",
    "in a json file:\n",
    "- id:\n",
    "- name:\n",
    "- level:\n",
    "- institution_id:\n",
    "- accredited_day:\n",
    "- accredited_month:\n",
    "- accredited_year:\n",
    "- expiry_day:\n",
    "- expiry_month:\n",
    "- expiry_year:\n",
    "- status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "url4 = 'https://unche.or.ug/all-academic-programs/'\n",
    "url4_text = requests.get('https://unche.or.ug/all-academic-programs/').text\n",
    "soup = bs(url4_text,'html.parser')\n",
    "program_table = soup.find('table', id = 'unche-table')\n",
    "headings =[th.text.strip() for th in program_table.tr.find_all('th')]\n",
    "\n",
    "program_data = []\n",
    "for index,row in enumerate(program_table.tbody.find_all('tr')):\n",
    "    daraa = [td.text.strip() for td in row.find_all('td')]\n",
    "    program_data.append(daraa)\n",
    "    \n",
    "dirty_df4 = pd.DataFrame(data=program_data , columns = headings)\n",
    "dirty_df4['Accredited Date'] = pd.to_datetime(dirty_df4['Accredited Date'], infer_datetime_format = True)\n",
    "dirty_df4['Accredited_Day'] =dirty_df4['Accredited Date'].dt.day\n",
    "dirty_df4['Accredited_Month'] =dirty_df4['Accredited Date'].dt.month\n",
    "dirty_df4['Accredited_Year'] =dirty_df4['Accredited Date'].dt.year\n",
    "dirty_df4['Expiry Date'] = pd.to_datetime(dirty_df4['Expiry Date'], infer_datetime_format = True)\n",
    "dirty_df4['Expiry_Day'] =dirty_df4['Expiry Date'].dt.day\n",
    "dirty_df4['Expiry_Month'] =dirty_df4['Expiry Date'].dt.month\n",
    "dirty_df4['Expiry_Year'] =dirty_df4['Expiry Date'].dt.year\n",
    "\n",
    "def level (program):\n",
    "    if program[:3] == 'PhD' or ('Doctor' in program):\n",
    "        return 'PhD'\n",
    "    elif program[:4] == 'Diploma' or ('Diploma' in program):\n",
    "        if 'Postgraduate' in program or ('Higher' in program) or ('Advanced' in program):\n",
    "            return 'Postgraduate'\n",
    "        return 'Diploma'\n",
    "    elif  'Bachelor' in program:\n",
    "        return 'Bachelor'\n",
    "    elif  'Master' in program:\n",
    "        return 'Masters'\n",
    "    elif 'Certificate' in program:\n",
    "        return 'Certificate'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "    \n",
    "dirty_df4 = dirty_df4.reset_index().rename(columns = ({'index': 'Id'}))\n",
    "dirty_df4['level'] = dirty_df4['Program Name'].apply(level)\n",
    "dirty_df4.rename(columns = ({'Program Name':'Name','Institution Name':'Institution'}),inplace = True)\n",
    "dirty_df4.drop(columns =['Accredited Date','Expiry Date'],inplace =True)\n",
    "colss =['Id', 'Name', 'level','Institution','Accredited_Day','Accredited_Month','Accredited_Year','Expiry_Day','Expiry_Month','Expiry_Year','Status']\n",
    "dirty3 = dirty_df4[colss]\n",
    "programs = dirty3.to_json(orient = 'records')\n",
    "with open('programs.json','w+') as f:\n",
    "    f.write(programs)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
