{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEBSCRAPING PROJECT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![WEB IMAGE](web-scraping-with-python.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION\n",
    "Webscrapping is the process of extracting data from a website and tranforming this data to a more useful structure that can \n",
    "uderstood by a user or an API.\n",
    "Webscrapping is one of the recquired skills  in the data collection stage and therefore this project is a simple task that i  \n",
    "have taken to understand webscrapping.\n",
    "It involves scrapping wikipedia pages and extracting some information which is then stored in a json file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTING PACKAGES\n",
    "In this stage I import all the necessary packages that i need to work out this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup as bs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REGIONS\n",
    "In this first section, i scrape through the wikipedia website containing regions in uganda and i save the result in a \n",
    "json file containing \n",
    "- id:\n",
    "- Region:\n",
    "- area:\n",
    "- chief_town:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'https://en.wikipedia.org/wiki/Regions_of_Uganda'\n",
    "url1_text = requests.get('https://en.wikipedia.org/wiki/Regions_of_Uganda').text\n",
    "soup = bs(url1_text, 'html.parser')\n",
    "table = soup.find('table', class_ = 'wikitable sortable')\n",
    "headers = [th.text.strip() for th in table.tr.find_all('th')]\n",
    "\n",
    "lst_data = []\n",
    "for index,row in enumerate(table.tbody.find_all('tr')[1:5]):\n",
    "    data = [td.text.strip() for td in row.find_all('td')]\n",
    "    lst_data.append(data)\n",
    "    \n",
    "dirty_df = pd.DataFrame(data=lst_data, columns=headers)\n",
    "dirty_df.drop(columns=['Population(Census 1991)','Population(Census 2002)','Population(Census 2014)','Number of Districts'],inplace=True)\n",
    "Region_df = dirty_df.reset_index().rename(columns = ({'index':'Reg_ID'}))\n",
    "Region_df.set_index('Reg_ID',inplace = True)\n",
    "regions = Region_df.to_json(orient = 'columns')\n",
    "\n",
    "with open('regions.json','w+') as f:\n",
    "    f.write(regions)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DISTRICTS\n",
    "In this section, i scrape through a wikipedia page containing a number of tables on districts in Uganda and save the data\n",
    "to a json file containing:\n",
    "- id\n",
    "- name\n",
    "- region_id\n",
    "- 2014_population\n",
    "- 2023_population_est\n",
    "- area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = 'https://en.wikipedia.org/wiki/Districts_of_Uganda'\n",
    "url2_text = requests.get('https://en.wikipedia.org/wiki/Districts_of_Uganda').text\n",
    "soup = bs(url2_text,'html.parser')\n",
    "dis_data = []\n",
    "for table in soup.find_all('table','wikitable sortable'):\n",
    "    headerss = [th.text.strip() for th in table.tbody.tr.find_all('th')]\n",
    "    for index,row in enumerate(table.tbody.find_all('tr')[1:-1]):\n",
    "        data = [td.text.strip() for td in row.find_all('td')]\n",
    "        dis_data.append(data)\n",
    "dirty_df2 = pd.DataFrame(data =dis_data,columns =headerss)\n",
    "dirty_df2.rename(columns={'Map':'District_ID'},inplace = True)\n",
    "dirty_df2['District_ID'] = dirty_df2['District_ID'].astype('int')\n",
    "dirty_df2.sort_values(by='District_ID',ascending = True, inplace = True, ignore_index = True)\n",
    "dirty_df2\n",
    "def Region_Identification(district):\n",
    "    if district <=38 :\n",
    "        return '3'\n",
    "    if district == 39 or district <= 75:\n",
    "        return '2'\n",
    "    if district == 76 or district <= 101:\n",
    "        return '0'\n",
    "    if district > 101:\n",
    "        return '1'\n",
    "        \n",
    "dirty_df2['Reg_ID'] = dirty_df2['District_ID'].apply(Region_Identification)\n",
    "dirty_df2.set_index('District_ID',inplace = True)\n",
    "dirty_df2\n",
    "districts = dirty_df2.to_json(orient='columns')\n",
    "with open('districts.json','w+') as f:\n",
    "    f.write(districts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTITUTIIONS\n",
    "In this project, i scrape through wikipedia pages of different institutions and save the following information in \n",
    "json file.\n",
    "- id\n",
    "- name:\n",
    "- district_id:\n",
    "- ownership_type: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "url3 = 'https://unche.or.ug/institutions/'\n",
    "url3_text = requests.get('https://unche.or.ug/institutions/').text\n",
    "soup = bs(url3_text,'html.parser')\n",
    "insti_table = soup.find('table', id = 'unche-table')\n",
    "headersss =[th.text.strip() for th in insti_table.tr.find_all('th')]\n",
    "\n",
    "institution_data = []\n",
    "for index,row in enumerate(insti_table.tbody.find_all('tr')):\n",
    "    dara = [td.text.strip() for td in row.find_all('td')]\n",
    "    institution_data.append(dara)\n",
    "\n",
    "dirty_df3 = pd.DataFrame(data = institution_data, columns = headersss)\n",
    "dirty_df3 = dirty_df3.reset_index().rename(columns = ({'index':'id'}))\n",
    "dirty_df3.drop(['Award Type','Programs'],axis =1 ,inplace = True)\n",
    "institutions = dirty_df3.to_json(orient='columns')\n",
    "with open('institutions.json','w+') as f:\n",
    "    f.write(institutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROGRAMS \n",
    "In this section a i scrape through the wikipedia page containing different programs and save the following information \n",
    "in a json file:\n",
    "- id:\n",
    "- name:\n",
    "- level:\n",
    "- institution_id:\n",
    "- accredited_day:\n",
    "- accredited_month:\n",
    "- accredited_year:\n",
    "- expiry_day:\n",
    "- expiry_month:\n",
    "- expiry_year:\n",
    "- status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "url4 = 'https://unche.or.ug/all-academic-programs/'\n",
    "url4_text = requests.get('https://unche.or.ug/all-academic-programs/').text\n",
    "soup = bs(url4_text,'html.parser')\n",
    "program_table = soup.find('table', id = 'unche-table')\n",
    "headings =[th.text.strip() for th in program_table.tr.find_all('th')]\n",
    "\n",
    "program_data = []\n",
    "for index,row in enumerate(program_table.tbody.find_all('tr')):\n",
    "    daraa = [td.text.strip() for td in row.find_all('td')]\n",
    "    program_data.append(daraa)\n",
    "    \n",
    "dirty_df4 = pd.DataFrame(data=program_data , columns = headings)\n",
    "dirty_df4['Accredited Date'] = pd.to_datetime(dirty_df4['Accredited Date'], infer_datetime_format = True)\n",
    "dirty_df4['Accredited Day'] =dirty_df4['Accredited Date'].dt.day\n",
    "dirty_df4['Accredited Month'] =dirty_df4['Accredited Date'].dt.month\n",
    "dirty_df4['Accredited Year'] =dirty_df4['Accredited Date'].dt.year\n",
    "dirty_df4['Expiry Date'] = pd.to_datetime(dirty_df4['Expiry Date'], infer_datetime_format = True)\n",
    "dirty_df4['Expiry Day'] =dirty_df4['Expiry Date'].dt.day\n",
    "dirty_df4['Expiry Month'] =dirty_df4['Expiry Date'].dt.month\n",
    "dirty_df4['Expiry Year'] =dirty_df4['Expiry Date'].dt.year\n",
    "dirty_df4 = dirty_df4.reset_index().rename(columns = ({'index': 'Id'}))\n",
    "programs = dirty_df4.to_json(orient = 'columns')\n",
    "with open('programs.json','w+') as f:\n",
    "    f.write(programs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
